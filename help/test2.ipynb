{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "import os\n",
    "\n",
    "import praw\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from itertools import chain\n",
    "from openai import OpenAI\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "load_dotenv()\n",
    "class RedditKeywordMatcher:\n",
    "    def __init__(self, client_id: str, client_secret: str, user_agent: str):\n",
    "        \"\"\"\n",
    "        Initialize Reddit client and TF-IDF vectorizer\n",
    "        \n",
    "        Args:\n",
    "            client_id (str): Reddit API client ID\n",
    "            client_secret (str): Reddit API client secret\n",
    "            user_agent (str): Reddit API user agent\n",
    "        \"\"\"\n",
    "    \n",
    "\n",
    "        self.reddit = praw.Reddit(\n",
    "            client_id=client_id,\n",
    "            client_secret=client_secret,\n",
    "            user_agent=user_agent\n",
    "        )\n",
    "\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            stop_words='english',\n",
    "            max_features=5000,\n",
    "            ngram_range=(1, 2)\n",
    "        )\n",
    "        \n",
    "    def fetch_subreddit_posts(self, subreddit_name: str, limit: int = 100) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Fetch posts from a subreddit\n",
    "        \n",
    "        Args:\n",
    "            subreddit_name (str): Name of the subreddit\n",
    "            limit (int): Maximum number of posts to fetch\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: List of posts with their details\n",
    "        \"\"\"\n",
    "        subreddit = self.reddit.subreddit(subreddit_name)\n",
    "        posts = []\n",
    "        \n",
    "        for post in subreddit.hot(limit=limit):\n",
    "            posts.append({\n",
    "                'title': post.title,\n",
    "                'body': post.selftext,\n",
    "                'url': f\"https://reddit.com{post.permalink}\",\n",
    "                'score': post.score,\n",
    "                'created_utc': datetime.fromtimestamp(post.created_utc),\n",
    "                'num_comments': post.num_comments\n",
    "            })\n",
    "        \n",
    "        return posts\n",
    "    \n",
    "    def find_relevant_posts(self, \n",
    "                          keywords: List[str], \n",
    "                          subreddits: List[str], \n",
    "                          limit: int = 100, \n",
    "                          min_similarity: float = 0.1) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Find posts relevant to given keywords across specified subreddits\n",
    "        \n",
    "        Args:\n",
    "            keywords (List[str]): List of keywords to match\n",
    "            subreddits (List[str]): List of subreddits to search\n",
    "            limit (int): Maximum posts to fetch per subreddit\n",
    "            min_similarity (float): Minimum cosine similarity score to consider\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: Sorted dataframe of relevant posts\n",
    "        \"\"\"\n",
    "        all_posts = []\n",
    "        \n",
    "        # Fetch posts from all specified subreddits\n",
    "        for subreddit in subreddits:\n",
    "            try:\n",
    "                posts = self.fetch_subreddit_posts(subreddit, limit)\n",
    "                all_posts.extend(posts)\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching posts from r/{subreddit}: {str(e)}\")\n",
    "        \n",
    "        if not all_posts:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Combine title and body for text analysis\n",
    "        posts_text = [f\"{post['title']} {post['body']}\" for post in all_posts]\n",
    "        \n",
    "        # Create keyword query\n",
    "        keyword_query = ' '.join(keywords)\n",
    "        \n",
    "        # Vectorize posts and keywords\n",
    "        tfidf_matrix = self.vectorizer.fit_transform(posts_text + [keyword_query])\n",
    "        \n",
    "        # Calculate similarity scores\n",
    "        similarity_scores = cosine_similarity(\n",
    "            tfidf_matrix[-1:], \n",
    "            tfidf_matrix[:-1]\n",
    "        )[0]\n",
    "        \n",
    "        # Create DataFrame with results\n",
    "        results_df = pd.DataFrame(all_posts)\n",
    "        results_df['similarity_score'] = similarity_scores\n",
    "        \n",
    "        # Filter and sort results\n",
    "        relevant_posts = results_df[results_df['similarity_score'] >= min_similarity]\n",
    "        relevant_posts = relevant_posts.sort_values(\n",
    "            by=['similarity_score', 'score'], \n",
    "            ascending=[False, False]\n",
    "        )\n",
    "        \n",
    "        return relevant_posts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def main():\n",
    "    # Replace these with your Reddit API credentials\n",
    "    client_id=os.getenv(\"CLIENT_ID\"),\n",
    "    client_secret=os.getenv(\"CLIENT_SECRET\"),\n",
    "    user_agent=os.getenv(\"USER_AGENT\")\n",
    "    \n",
    "    # Initialize the matcher\n",
    "    matcher = RedditKeywordMatcher(client_id, client_secret, user_agent)\n",
    "    \n",
    "    # Example usage\n",
    "    keywords = [\"python\", \"machine learning\", \"data science\"]\n",
    "    subreddits = [\"programming\", \"Python\", \"learnpython\", \"datascience\"]\n",
    "    \n",
    "    results = matcher.find_relevant_posts(\n",
    "        keywords=keywords,\n",
    "        subreddits=subreddits,\n",
    "        limit=100,\n",
    "        min_similarity=0.1\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    if not results.empty:\n",
    "        print(f\"\\nFound {len(results)} relevant posts:\")\n",
    "        for idx, post in results.iterrows():\n",
    "            print(f\"\\nTitle: {post['title']}\")\n",
    "            print(f\"Relevance Score: {post['similarity_score']:.3f}\")\n",
    "            print(f\"Reddit Score: {post['score']}\")\n",
    "            print(f\"URL: {post['url']}\")\n",
    "            print(\"-\" * 80)\n",
    "    else:\n",
    "        print(\"No relevant posts found.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec5323d716488a035302055a45aa7bbb61e08eee7e154039f00dbb795e5b0e6c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
