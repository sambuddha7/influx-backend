{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "from datetime import datetime\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=os.getenv(\"CLIENT_ID\"),\n",
    "    client_secret=os.getenv(\"CLIENT_SECRET\"),\n",
    "    user_agent=os.getenv(\"USER_AGENT\"),\n",
    ")\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words='english',\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 3),  # Include up to 3-word phrases\n",
    "    analyzer='word',\n",
    "    token_pattern=r'(?u)\\b\\w+\\b'  # Include single-character words\n",
    ")\n",
    "        \n",
    "def fetch_reddit_posts(search_query: str, limit) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Fetch posts from all of Reddit based on search query\n",
    "    \n",
    "    Args:\n",
    "        search_query (str): Search query string\n",
    "        limit (int): Maximum number of posts to fetch\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: List of posts with their details\n",
    "    \"\"\"\n",
    "    posts = []\n",
    "    \n",
    "    # Search across all of Reddit\n",
    "    for submission in reddit.subreddit(\"all\").search(\n",
    "        search_query, \n",
    "        sort='relevance', \n",
    "        time_filter='month',\n",
    "        limit=limit\n",
    "    ):\n",
    "        posts.append({\n",
    "            'title': submission.title,\n",
    "            'body': submission.selftext,\n",
    "            'url': f\"https://reddit.com{submission.permalink}\",\n",
    "            'score': submission.score,\n",
    "            'created_utc': datetime.fromtimestamp(submission.created_utc),\n",
    "            'num_comments': submission.num_comments,\n",
    "            'subreddit': submission.subreddit.display_name\n",
    "        })\n",
    "    \n",
    "    return posts\n",
    "\n",
    "def find_relevant_posts(keywords: List[str], \n",
    "                        limit, \n",
    "                        min_similarity) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Find posts relevant to given keywords across all of Reddit\n",
    "    \n",
    "    Args:\n",
    "        keywords (List[str]): List of keywords to match\n",
    "        limit (int): Maximum posts to fetch\n",
    "        min_similarity (float): Minimum cosine similarity score to consider\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Sorted dataframe of relevant posts\n",
    "    \"\"\"\n",
    "    # Create search query from keywords\n",
    "    search_query = ' OR '.join(f'\"{kw}\"' for kw in keywords)\n",
    "    \n",
    "    # Fetch posts using Reddit's search\n",
    "    all_posts = fetch_reddit_posts(search_query, limit=limit)\n",
    "    \n",
    "    if not all_posts:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Combine title and body for text analysis\n",
    "    posts_text = [f\"{post['title']} {post['body']}\" for post in all_posts]\n",
    "    \n",
    "    # Create keyword query for TF-IDF\n",
    "    keyword_query = ' '.join(keywords)\n",
    "    \n",
    "    # Vectorize posts and keywords\n",
    "    tfidf_matrix = vectorizer.fit_transform(posts_text + [keyword_query])\n",
    "    \n",
    "    # Calculate similarity scores\n",
    "    similarity_scores = cosine_similarity(\n",
    "        tfidf_matrix[-1:], \n",
    "        tfidf_matrix[:-1]\n",
    "    )[0]\n",
    "    \n",
    "    # Create DataFrame with results\n",
    "    results_df = pd.DataFrame(all_posts)\n",
    "    results_df['similarity_score'] = similarity_scores\n",
    "    \n",
    "    # Filter and sort results\n",
    "    relevant_posts = results_df[results_df['similarity_score'] >= min_similarity]\n",
    "    relevant_posts = relevant_posts.sort_values(\n",
    "        by=['similarity_score', 'score'], \n",
    "        ascending=[False, False]\n",
    "    )\n",
    "    \n",
    "    return relevant_posts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 5 relevant posts:\n",
      "\n",
      "Title: Kriti Sanon Skin Care...\n",
      "Subreddit: r/KritiSanonn\n",
      "Relevance Score: 0.503\n",
      "Reddit Score: 88\n",
      "URL: https://reddit.com/r/KritiSanonn/comments/1ho52s0/kriti_sanon_skin_care/\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Which stream is better, homeopathy or ayurveda \n",
      "Subreddit: r/MEDICOreTARDS\n",
      "Relevance Score: 0.398\n",
      "Reddit Score: 1\n",
      "URL: https://reddit.com/r/MEDICOreTARDS/comments/1hv0d3z/which_stream_is_better_homeopathy_or_ayurveda/\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Has anyone tried Ayurveda ?\n",
      "Subreddit: r/UlcerativeColitis\n",
      "Relevance Score: 0.333\n",
      "Reddit Score: 0\n",
      "URL: https://reddit.com/r/UlcerativeColitis/comments/1haz610/has_anyone_tried_ayurveda/\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Whatâ€™s Ayurveda Like in the EU? Letâ€™s Chat! ðŸŒ¿âœ¨\n",
      "Subreddit: r/Ayurveda\n",
      "Relevance Score: 0.310\n",
      "Reddit Score: 8\n",
      "URL: https://reddit.com/r/Ayurveda/comments/1hpjqwl/whats_ayurveda_like_in_the_eu_lets_chat/\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Ayurveda Books recommendation for Hashimoto\n",
      "Subreddit: r/Ayurveda\n",
      "Relevance Score: 0.305\n",
      "Reddit Score: 5\n",
      "URL: https://reddit.com/r/Ayurveda/comments/1hoxt7b/ayurveda_books_recommendation_for_hashimoto/\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "keywords = [\"ayurveda\", \"skin care\"]\n",
    "\n",
    "results = find_relevant_posts(\n",
    "    keywords=keywords,\n",
    "    limit=10000,\n",
    "    min_similarity=0.3\n",
    ")\n",
    "\n",
    "if not results.empty:\n",
    "    print(f\"\\nFound {len(results)} relevant posts:\")\n",
    "    for idx, post in results.iterrows():\n",
    "        print(f\"\\nTitle: {post['title']}\")\n",
    "        print(f\"Subreddit: r/{post['subreddit']}\")\n",
    "        print(f\"Relevance Score: {post['similarity_score']:.3f}\")\n",
    "        print(f\"Reddit Score: {post['score']}\")\n",
    "        print(f\"URL: {post['url']}\")\n",
    "        print(\"-\" * 80)\n",
    "else:\n",
    "    print(\"No relevant posts found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "reddit_instance = praw.Reddit(\n",
    "    client_id=os.getenv(\"CLIENT_ID\"),\n",
    "    client_secret=os.getenv(\"CLIENT_SECRET\"),\n",
    "    user_agent=os.getenv(\"USER_AGENT\"),\n",
    ")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and standardize text for better matching\"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters but keep spaces between words\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "def search_reddit(reddit_instance, keywords, limit, min_similarity):\n",
    "    \"\"\"\n",
    "    Search Reddit for posts matching keywords\n",
    "    Keywords can be single words or phrases\n",
    "    \"\"\"\n",
    "    # Preprocess keywords\n",
    "    processed_keywords = [preprocess_text(kw) for kw in keywords]\n",
    "    \n",
    "    # Create Reddit search query (keep original format for Reddit's search)\n",
    "    search_query = ' OR '.join(f'\"{kw}\"' for kw in keywords)\n",
    "    posts = []\n",
    "\n",
    "    try:\n",
    "        print(\"Fetching posts from Reddit...\")\n",
    "        for submission in reddit_instance.subreddit(\"all\").search(\n",
    "            search_query,\n",
    "            sort='relevance',\n",
    "            time_filter='month',\n",
    "            limit=limit\n",
    "        ):\n",
    "            # Preprocess title and body\n",
    "            processed_title = preprocess_text(submission.title)\n",
    "            processed_body = preprocess_text(submission.selftext)\n",
    "            \n",
    "            # Calculate initial keyword matches\n",
    "            title_matches = sum(1 for kw in processed_keywords if kw in processed_title)\n",
    "            body_matches = sum(1 for kw in processed_keywords if kw in processed_body)\n",
    "            \n",
    "            posts.append({\n",
    "                'title': submission.title,\n",
    "                'body': submission.selftext,\n",
    "                'processed_text': f\"{processed_title} {processed_body}\",\n",
    "                'url': f\"https://reddit.com{submission.permalink}\",\n",
    "                'score': submission.score,\n",
    "                'created_utc': datetime.fromtimestamp(submission.created_utc),\n",
    "                'num_comments': submission.num_comments,\n",
    "                'subreddit': submission.subreddit.display_name,\n",
    "                'keyword_matches': title_matches + body_matches\n",
    "            })\n",
    "\n",
    "        if not posts:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # Prepare documents for TF-IDF\n",
    "        # Include both original keywords and their individual words\n",
    "        keyword_docs = []\n",
    "        for kw in keywords:\n",
    "            # Add the full phrase\n",
    "            keyword_docs.append(kw)\n",
    "        \n",
    "        # Combine all keyword variations into one query document\n",
    "        keyword_query = ' '.join(keyword_docs)\n",
    "        \n",
    "        # Get processed text from posts\n",
    "        posts_text = [post['processed_text'] for post in posts]\n",
    "\n",
    "        # Initialize TF-IDF vectorizer with specific settings for multi-word handling\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            stop_words='english',\n",
    "            max_features=5000,\n",
    "            ngram_range=(1, 3),  # Include up to 3-word phrases\n",
    "            analyzer='word',\n",
    "            token_pattern=r'(?u)\\b\\w+\\b'  # Include single-character words\n",
    "        )\n",
    "\n",
    "        # Vectorize posts and keywords\n",
    "        tfidf_matrix = vectorizer.fit_transform(posts_text + [keyword_query])\n",
    "        \n",
    "        # Calculate similarity scores\n",
    "        similarity_scores = cosine_similarity(\n",
    "            tfidf_matrix[-1:], \n",
    "            tfidf_matrix[:-1]\n",
    "        )[0]\n",
    "\n",
    "        # Create DataFrame with results\n",
    "        results_df = pd.DataFrame(posts)\n",
    "        results_df['similarity_score'] = similarity_scores\n",
    "        \n",
    "        # Adjust similarity scores based on exact keyword matches\n",
    "        results_df['adjusted_score'] = results_df.apply(\n",
    "            lambda x: x['similarity_score'] * (1 + 0.1 * x['keyword_matches']), \n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        # Filter and sort results\n",
    "        min_similarity = min_similarity\n",
    "        relevant_posts = results_df[results_df['adjusted_score'] >= min_similarity]\n",
    "        return relevant_posts.sort_values(\n",
    "            by=['adjusted_score', 'score'], \n",
    "            ascending=[False, False]\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching posts from Reddit...\n",
      "\n",
      "Top matching posts:\n",
      "\n",
      "Title: What are the key steps to effectively manage HIPAA compliance in a small healthcare practice?\n",
      "Relevance Score: 0.365\n",
      "Keyword Matches: 2\n",
      "URL: https://reddit.com/r/hipaa/comments/1hkljvt/what_are_the_key_steps_to_effectively_manage/\n",
      "\n",
      "Title: Texas A&M University-Mays Business School and Humana Announce Winners of Eighth Annual Healthcare Analytics Case Competition\n",
      "Relevance Score: 0.338\n",
      "Keyword Matches: 1\n",
      "URL: https://reddit.com/r/Quantisnow/comments/1hhv4u3/texas_am_universitymays_business_school_and/\n",
      "\n",
      "Title: Are you in Healthcare analytics role ?\n",
      "Relevance Score: 0.334\n",
      "Keyword Matches: 1\n",
      "URL: https://reddit.com/r/ITCareerQuestions/comments/1hlkf07/are_you_in_healthcare_analytics_role/\n",
      "\n",
      "Title: FORA | Forian Partners With Databricks to Expand Access to Advanced Healthcare Analytics\n",
      "Relevance Score: 0.331\n",
      "Keyword Matches: 1\n",
      "URL: https://reddit.com/r/StockTitan/comments/1hfrkho/fora_forian_partners_with_databricks_to_expand/\n",
      "\n",
      "Title: Forian Partners With Databricks to Expand Access to Advanced Healthcare Analytics\n",
      "Relevance Score: 0.331\n",
      "Keyword Matches: 1\n",
      "URL: https://reddit.com/r/Quantisnow/comments/1hfilqh/forian_partners_with_databricks_to_expand_access/\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "keywords = [\"Patient portal\", \"HIPAA compliance\", \"Virtual consultations\", \"Healthcare analytics\"]\n",
    "results = search_reddit(reddit, keywords,10000, 0.3)\n",
    "\n",
    "if not results.empty:\n",
    "    print(\"\\nTop matching posts:\")\n",
    "    for idx, post in results.iterrows():\n",
    "        print(f\"\\nTitle: {post['title']}\")\n",
    "        print(f\"Relevance Score: {post['adjusted_score']:.3f}\")\n",
    "        print(f\"Keyword Matches: {post['keyword_matches']}\")\n",
    "        print(f\"URL: {post['url']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (default, Sep 16 2021, 08:50:36) \n[Clang 10.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec5323d716488a035302055a45aa7bbb61e08eee7e154039f00dbb795e5b0e6c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
